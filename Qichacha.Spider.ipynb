{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/HongX/anaconda3/bin/python\r\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named bs4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7850c7981af9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named bs4"
     ]
    }
   ],
   "source": [
    "# -*- coding: gbk -*-\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import xlrd\n",
    "import xlwt\n",
    "from xlutils.copy import copy\n",
    "import time\n",
    "import winsound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#企查查网站爬虫类\n",
    "class EnterpriseInfoSpider:\n",
    "    def __init__(self):\n",
    "\n",
    "        #文件相关\n",
    "        self.excelPath = 'enterprise_data.xls'\n",
    "        self.sheetName = 'details'\n",
    "        self.workbook = None\n",
    "        self.table = None\n",
    "        self.beginRow = None\n",
    "\n",
    "        # 目录页\n",
    "        self.catalogUrl = \"http://www.qichacha.com/search_index\"\n",
    "\n",
    "        # 详情页（前缀+firmXXXX+后缀）\n",
    "        self.detailsUrl = \"http://www.qichacha.com/company_getinfos\"\n",
    "\n",
    "        self.cookie = raw_input(\"请输入cookie:\").decode(\"gbk\").encode(\"utf-8\")\n",
    "        self.host = \"www.qichacha.com\"\n",
    "        self.userAgent = \"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/51.0.2704.106 Safari/537.36\"\n",
    "\n",
    "        self.headers = {\n",
    "            \"cookie\" : self.cookie,\n",
    "            \"host\" : self.host,\n",
    "            \"user-agent\" : self.userAgent\n",
    "        }\n",
    "\n",
    "        #数据字段名17个\n",
    "        self.fields = ['公司名称','电话号码','邮箱','统一社会信用代码','注册号','组织机构代码','经营状态','公司类型','成立日期','法定代表人','注册资本',\n",
    "                       '营业期限','登记机关','发照日期','公司规模','所属行业','英文名','曾用名','企业地址','经营范围']\n",
    "\n",
    "    #爬虫开始前的一些预处理\n",
    "    def init(self):\n",
    "\n",
    "        try:\n",
    "            #试探是否有该excel文件，#获取行数：workbook.sheets()[0].nrows\n",
    "            readWorkbook = xlrd.open_workbook(self.excelPath)\n",
    "            self.beginRow = readWorkbook.sheets()[0].nrows #获取行数\n",
    "            self.workbook = copy(readWorkbook)\n",
    "            self.table = self.workbook.get_sheet(0)\n",
    "\n",
    "        except Exception,e:\n",
    "            print e\n",
    "            self.workbook = xlwt.Workbook(encoding='utf-8')\n",
    "            self.table = self.workbook.add_sheet(self.sheetName)\n",
    "\n",
    "            #创建表头字段\n",
    "            col = 0\n",
    "            for field in self.fields:\n",
    "                self.table.write(0,col,field.decode('gbk').encode('utf-8'))\n",
    "                col += 1\n",
    "\n",
    "            self.workbook.save(self.excelPath)\n",
    "            self.beginRow = 1\n",
    "            print \"已在当前目录下创建enterprise_data.xls数据表\"\n",
    "\n",
    "\n",
    "    #从keyword/1页 得到的html中获得总页码数\n",
    "    def getTotalPage(self,catalogPageCode):\n",
    "        soup = BeautifulSoup(catalogPageCode,\"html.parser\")\n",
    "        pagebar = soup.select(\"li #ajaxpage\")\n",
    "        if pagebar == None or pagebar == []:\n",
    "            return -1\n",
    "        return int(soup.select(\"li #ajaxpage\")[-1].string.strip(' .'))\n",
    "\n",
    "    #从keyword/page页 得到html中的所有公司条目\n",
    "    def getFirmIdDoms(self,catalogPageCode):\n",
    "        soup = BeautifulSoup(catalogPageCode,\"html.parser\")\n",
    "        return soup.select(\"#searchlist .table-search-list .tp2 a\")\n",
    "\n",
    "    #爬虫开始\n",
    "    def start(self):\n",
    "        keyword = raw_input(\"请输入关键字：\").decode(\"gbk\").encode(\"utf-8\")\n",
    "        while keyword != \"end\":\n",
    "            #先获取keyword第一页内容的页码\n",
    "            totalPage = self.getTotalPage(self.getCatalogPageCode(keyword, 1))\n",
    "            if totalPage == -1:\n",
    "                # 请求下一轮查询的关键字\n",
    "                keyword = raw_input(\"爬取结束,请输入关键字：\").decode(\"gbk\").encode(\"utf-8\")\n",
    "                continue\n",
    "\n",
    "            #模拟翻页操作\n",
    "            for page in range(1,totalPage+1):\n",
    "\n",
    "                print \"正在爬取第\",page,\"页的数据,请稍等...\"\n",
    "\n",
    "                #获取第page页代码\n",
    "                catalogPageCode = self.getCatalogPageCode(keyword,page)\n",
    "                firmIdDoms = self.getFirmIdDoms(catalogPageCode)\n",
    "                for firmIdDom in firmIdDoms:\n",
    "                    firmId = firmIdDom['href'][6:-6]\n",
    "                    companyname = \"\" #公司名\n",
    "                    for string in firmIdDom.strings:\n",
    "                        companyname += string\n",
    "\n",
    "                    tdDom = firmIdDom.find_parent().find_parent()\n",
    "                    phoneDom = tdDom.select('.i-phone3')\n",
    "                    emailDom = tdDom.select('.fa-envelope-o')\n",
    "                    phone = \"\"\n",
    "                    email = \"\"\n",
    "                    if phoneDom != None and phoneDom != []:\n",
    "                        phone = phoneDom[0].next_sibling.strip() #手机\n",
    "                    if emailDom != None and emailDom != []:\n",
    "                        email = emailDom[0].next_sibling.strip() #邮箱\n",
    "\n",
    "                    detailsPageCode = self.getDetailsPageCode(firmId,companyname)\n",
    "                    self.writeDetailsToExcel(detailsPageCode,companyname,phone,email)\n",
    "                    time.sleep(0.3) #0.5s后再爬防止反爬虫机制\n",
    "\n",
    "            #请求下一轮查询的关键字\n",
    "            keyword = raw_input(\"爬取结束,请输入关键字：\").decode(\"gbk\").encode(\"utf-8\")\n",
    "\n",
    "        print \"爬虫已完全结束！\"\n",
    "\n",
    "    #根据keyword和page构造查询串\n",
    "    #其中keyword中的空格换成+\n",
    "    #返回查询字符串构成的字典\n",
    "    def getCatalogQueryString(self,keyword,page):\n",
    "        keyword.replace(' ','+')\n",
    "        return {\"key\": keyword, \"index\": \"0\", \"p\": page}\n",
    "\n",
    "    def getDetailQueryString(self,firmId,companyname):\n",
    "        return {\"unique\": firmId, \"companyname\":companyname,\"tab\": \"base\"}\n",
    "\n",
    "    # 根据keyword关键字获取目录页代码\n",
    "    def getCatalogPageCode(self, keyword, page):\n",
    "        queryString = self.getCatalogQueryString(keyword, page)\n",
    "        response = requests.request(\"GET\", self.catalogUrl, headers=self.headers, params=queryString)\n",
    "        return response.text\n",
    "\n",
    "    # 根据firmId获取公司的详情页代码\n",
    "    def getDetailsPageCode(self,firmId,companyname):\n",
    "        queryString = self.getDetailQueryString(firmId,companyname)\n",
    "        response = requests.request(\"GET\", self.detailsUrl, headers=self.headers, params=queryString)\n",
    "        return response.text\n",
    "\n",
    "    #抓取detailsPageCode页上该企业所有信息，并存入excel\n",
    "    def writeDetailsToExcel(self,detailsPageCode,companyname,phone,email):\n",
    "        detailDoms = self.getDetailDoms(detailsPageCode)\n",
    "\n",
    "        self.table.write(self.beginRow, 0, companyname)\n",
    "        self.table.write(self.beginRow, 1, phone)\n",
    "        self.table.write(self.beginRow, 2, email)\n",
    "\n",
    "        col = 3\n",
    "        for detailDom in detailDoms:\n",
    "            detailName = detailDom.label.string.strip()[:-1]\n",
    "            detailValue = detailDom.label.next_sibling.string.strip()\n",
    "            while col < len(self.fields):\n",
    "                # 找到匹配的那列字段\n",
    "                if detailName == self.fields[col].decode('gbk'):\n",
    "                    self.table.write(self.beginRow, col, detailValue) #写入excel\n",
    "                    col += 1\n",
    "                    break\n",
    "                else:\n",
    "                    col += 1\n",
    "        self.workbook.save(self.excelPath)  # 保存至文件\n",
    "        self.beginRow += 1\n",
    "\n",
    "    #根据detailsPageCode获得它的所有detailDoms元素\n",
    "    def getDetailDoms(self,detailsPageCode):\n",
    "        soup = BeautifulSoup(detailsPageCode,\"html.parser\")\n",
    "        return soup.select(\".company-base li\")\n",
    "\n",
    "\n",
    "\n",
    "########\n",
    "#爬虫入口\n",
    "########\n",
    "spider = EnterpriseInfoSpider()\n",
    "spider.init()\n",
    "spider.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
